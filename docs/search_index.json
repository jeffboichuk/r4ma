[
["index.html", "R for Marketing Analytics Welcome", " R for Marketing Analytics Jeffrey Boichuk Steven Mortimer Welcome In this book we’ll cover how to perform quantitative marketing using the R programming language. Whether you are a student searching for real world examples of quantitative marketing or you are a seasoned marketing analyst looking for new methods or ideas to implement at your place of work this is the book for you. First, we’ll cover how to intelligently approach a problem. Second, we’ll go into detail on how to implement a solution using the R programming language. Lastly, we will discuss how to influence marketing decisions via effective methods and communication. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["preface.html", "Preface", " Preface Outline the structure of the book, introduce ourselves, etc. Jeffrey Boichuk () is … Steven Mortimer has more than 7 years experience in data science serving clients and businesses across the healthcare, higher education, and media industries. Steven was the first data scientist hired at two different companies where he grew data science as a corporate function. He specializes in team building, rapid prototyping, and practical applications of data science using the R programming language to quickly realize business value. Also, Steven is committed to helping others develop skills in data science. At the University of Virginia (UVA), he created content for a graduate course in Customer Analytics and tutors students needing assistance with statistics-based coursework. Much of Steven’s code is publicly available in his GitHub repositories at https://github.com/StevenMMortimer Steven holds Bachelor’s and Master’s degrees in Statistics from the University of Virginia and is currently pursing a Master’s of Business Administration at UVA’s Darden School of Business. "],
["marketing.html", "1 What is Marketing?", " 1 What is Marketing? What questions to ask, etc. "],
["tools.html", "2 Tools at your Disposal ", " 2 Tools at your Disposal "],
["data.html", "2.1 Data", " 2.1 Data List the types of data. When to use and why. "],
["software.html", "2.2 Software", " 2.2 Software There is no shortage of options when choosing software for analysis. A number of open source and proprietary tools have been introduced to make analyzing data easier. We think of these tools in three different categories: Business Intelligence (BI) Tools Visual Analysis Tools Programming Languages Business intelligence (BI) tools are mainly used for summarizing and visualizing data. Some of the most popular tools include: Tableau, Domo, Pentaho, and Looker. The audience for these tools are mainly decision-makers of the data, managers and executives who need insights quickly. For this reason, these tools usually operate with a fixed set of features on top of a standardized data source. Depending on the tool there may be functionality to clean and curate data, but they main focus is on presenting the insights. These tools are extremely powerful and valuable, but as an analyst you will typically need more control and flexibility when handling data. In contrast with BI tools, visual analysis tools are designed with the analyst in mind. They usually have functions to retrieve and transform data as well as visualize and analyze it for insights. Some tools include: Excel, SPSS, RapidMiner, and Minitab. The most ubiquitous is of these tools is Excel. In this book we will draw analogies of how certain operations in Excel translate into R. These tools are great for ad hoc analysis because they are easy to maniupulate and you can literally see the data and how it changes throughout your analysis. This is a huge benefit to analysts because seeing the data makes it easier to spot anomolies and mistakes. Unfortunately, these tools do not make it easy to replicate analyses. Many of these tools are point-and-click systems so that whenever an action needs to be repeated it can take time to re-run and there is a risk of introducing errors. Finally, programming languages have more recently become a viable tool for analysis. With the explosion of data programming languages have developed libraries or modules specifically for handling data. They typically abstract some of the underlying code so that the analyst can focus on working with the data instead of fiddling with obtuse bits of code. Some tools include: Java, Python, Julia, Scala, and R. Prior experience with one of these languages is a good reason to begin using it for analysis. These tools have the steepest learning curve, so the more familiar you are with the syntax and structure, the faster you can begin analyzing data. For learners we recommend Python or R given they are at the forefront of data analysis and there are a number courses and online materials devoted to learning them. The tradeoff for being difficult to learn is that writing a script creates an artifact that is stable and lasting. It can be executed on demand and extended to have more functionality over time. Of all the tools mentioned in this section we recommend using a programming language for almost all of your analyses. "],
["getting-started-with-r.html", "2.3 Getting Started with R", " 2.3 Getting Started with R In this book we will further detail how the R programming language can be used for marketing analytics. We recommend the R language for a few reasons. First, it is an open source, cross-platform tool. This means that the software is free to download and use for academic and commercial purposes. It also runs on a variety of computer operating systems such as Windows, Mac, and Linux. This means that it is accessible to almost anyone. Second, the R language was designed specifically for analysis. Java, Python, and other programming languages have robust libraries to do analysis, but these are not the origin of the languge, so their documentation is often hard to read or unclear. In addition, examples on the internet are out of date or no longer work due to changes in versions. One reason why R is easier to learn and extend is that its development in recent years has been driven by the vision of Hadley Wickham. Hadley is a rockstar in the analytics community for his contributions to advancing the R programming language. He has created and introduced a style of programming and analysis based on “tidy” data principles. INSERT REFERENCE TO TIDY PAPER (Xie 2015). The tidyverse (Xie 2018) is a collection of multiple packages (libraries) designed or influenced by Hadley’s approach to analysis. These packages build upon each other so that it is easy to wrangle data into a tidy format and then proceed to analyze it. In order to get started you should install RStudio. This is the preferred application to write R code. After installing, open up RStudio and type the following into the bottom left pane entitled “Console”: install.packages(&#39;tidyverse&#39;) Pressing enter will run this command and a long stream of text and red messages will appear in the Console window. This is okay and usually a good thing. It means the messages indicate that the code library has been downloaded and unpackaged to your computer. Installing an R package is an important the first skill to learn. Note that the words “package” and “library” are interchangeable and both reference a collection of functions and code that you can download, install, and use in R. You will need to repeat the step of installing every new package that you want to use. You must install it, but only once. After it is installed, it will be available for you to use. However, you must “load” the library every time you start a new R session (e.g. open up RStudio). This step will make all the functions available in your session. In the Console window type library(tidyverse) to load the library. There will be a few messages that indicate the status of the library as you have loaded it. One way to check that the package is loaded is by … The other way is to click on the “Packages” tab in the bottom right pane in RStudio. There will be a list of every package you have installed, including the Base R packages, with a check mark next to each one that you have loaded in the current session. [INCLUDE IMAGE OF PACKAGE CHECK MARK] We have created a companion package with data for this book. You can install and load that package by running the following code: install.packages(&#39;completejourney&#39;) library(completejourney) We have instructed you to copy/paste or type this code into the Console window and press ENTER to execute it. This is okay for running a few small commands, but we recommend putting all of your commands into the top left pane, the Script Editor window of RStudio. This will be a self-contained reference of commands that you can run again whenever they are needed. You can send commands from the Script Editor window to the Console window by pressing CTRL+ENTER on Windows or CMD+ENTER on a Mac. The reason that there are two different panes in RStudio is that one is for composing your scripts and the other is for executing them. You can type as much as you would like in the Script Editor, but the variables and functions you write are created only when you execute the code. Again, we recommend writing your code in the editor pane and then executing it. Now that you are familiar with installing and loading libraries you know how to set up your R session. The next step is writing and executing your analysis. With programming langauges the flow is to execute functions and save their result into a variable to inspect it or re-use it in another way. In programming this is called “assignment”. You are assigning the result to an object in the session. We want to mention this now because R has a peculiar assignment operator that looks like this &lt;-. Quite literally this means take the object on the right and push it to an object on the left. For example, if we want to assign the average of numbers 1 through 10 to an object called “x” the code would look like this: x &lt;- mean(1:10) You will see most operations in R having this flow where a command is performed and the result is assigned to an object using &lt;-. Throughout the book we will introduce more functions with the assumption that you have a basic understanding of how to read the documentation for functions, run them, and assign the result to an object. For a more comprehensive guide to learning the basics of writing R code we recommend … [WHAT DO WE RECOMMEND?] OTHER IMPORTANT CONSIDERATIONS TO TEACH BEFORE DIVING INTO DATA VIZ? **** References "],
["dataviz.html", "3 Data Visualizations", " 3 Data Visualizations Good marketing tells a story, it evokes an emotion, and elicits a response. The same can be said about good data visualizations. In this chapter we will show you how to use the R package ggplot2 to visualize different types of marketing data to tell your story. "],
["creating-a-canvas.html", "3.1 Creating a Canvas", " 3.1 Creating a Canvas The ggplot2 package is an interesting solution because it does not provide a set of chart types that you can pick and choose. Microsoft Excel has always provided a fixed set of scattercharts, bar charts, and other popular visualizations. In contrast, ggplot2 an extensible system of commands that build literally any picture you’d like to create. Talented R users have created ggplot masterpieces, such as, the picture of Hadley. [ADD HADLEY PICTURE HERE] Every ggplot visualization starts with a canvas. You can create a blank canvas using the function ggplot(). This function takes two arguments: 1) data and 2) mapping. You can think of the data argument as a pallete of information from which to construct the plot and and the mapping as the plan for how to structure that information (e.g. as the x-axis, y-axis, size, shape or color). Supplying these argument still stop short of actually creating a plot library(tidyverse) #library(ourpackage) # ggplot(data=iris, mapping=aes()) "],
["adding-layers-of-geoms.html", "3.2 Adding Layers of Geoms", " 3.2 Adding Layers of Geoms Now that you’ve created a blank canvas, you need to add layers to it. In the canvas example above we used the aes() function, which is short for aesthetics. When you define the aesthetics in the top level ggplot() function call, it applies those aesthetics to every layer that subsequently gets applied to the plot. Layers is the core idea of constructing a plot. Every new layer gets added to the canvas via the plus sign (+). As an example, let’s add a set of points to the canvas. You can do this using the geom_point() function. # ggplot(data=iris, mapping=aes()) + # geom_point() A “geom” is shorthand for adding a layer to your plot of a specific geometric shape. Geoms are the heart of the plot because they define whether the plot is a line plot, scatterplot, or bar plot. In each of these examples, the geom would be geom_line(), geom_point() or geom_bar() and there are many more geom types to consider. The ggplot2 package tries to makes things intuitive and easy to remember. However, if you need a list of all the geoms you can find them using tab completion in RStudio. Just type geom_ in your RStudio console window , then press TAB. This will trigger the tab-completion feature in RStudio and show you the list of geoms that are available for you to use. [ADD TAB COMPLETION SCREENSHOT HERE] "],
["adding-labels-axes-and-legends.html", "3.3 Adding Labels, Axes, and Legends", " 3.3 Adding Labels, Axes, and Legends With plots it is important to pay attention to the details. These details make an effective plot and just like Microsoft Excel allows you to add and customize each element, so does ggplot. The trick is knowing which functions to use so in this section we will introduce a laundry list of them and then present all of them together to create a single polished plot. scale_x_continuous, scale_x_discrete scale_y_continuous, scale_y_continuous We highly recommend using the scales package when plotting. It comes with a variety of functions to make the scale of axes easier to read. labs() guide() "],
["faceting-data-by-group.html", "3.4 Faceting Data by Group", " 3.4 Faceting Data by Group A common research question is comparing a response across groups. The best way to visualize these relationships is to use the facet_grid() function. A facet creates a panel of plots with one for each value in the grouping variable. In the example below we have created a histogram for each Species in the iris dataset (NOTE: REPLACE WITH MKTING EXAMPLE). One important aspect to remember when creating facet charts is the scale of the axes. You can accidentally, and purposefully, mislead readers by having the same metric plotted on two different axis scales. IS THIS THE default behavior for the facet_grid() The syntax for facetting data has changed over time, from formula notation to a newer style. You may see examples online that use this older notation like so facet_grid(.~var). This is equivalent to facet_grid(cols=vars(var)). "],
["chart-types.html", "3.5 Chart Types", " 3.5 Chart Types Marketing Specific Charts? pie chart? a critique on pie charts "],
["advanced-plotting.html", "3.6 Advanced Plotting", " 3.6 Advanced Plotting theme_blank() ggthemes annotating? typically needing long format? gather? extentions? We would be remiss not to mention the amazing collection of open source “extensions” that make it easier to produce beautiful plots. These are ggextentions. "],
["datatransformation.html", "4 Preparing and Transforming Data", " 4 Preparing and Transforming Data It is a common adage that data analysts spend 80% of their time cleaning data and only 20% on value-add analysis. In this chapter we will show the basics of transforming data with the tidyverse package dplyr. Using these functions are the best way to reduce the overall amount of time spent on data cleaning and to create reproducible workflows whenever you receive new data. "],
["data-pipelines.html", "4.1 Data Pipelines", " 4.1 Data Pipelines Before diving into the functions designed to help you transform data, we must cover a perculiar looking operator, the pipe or %&gt;%. You will repeatedly see the pipe (%&gt;%) used in the code examples for this chapter and in other parts of the book. This operator takes output from the function on its left side and passes it onto the function on its right side. As an example we will look at the select() function. The first argument in the function is .data. Whenever you use the pipe operator it will pass that output into the function’s first argument, which in this case, is a tbl_df. This means that we must put a tbl_df object (a dataset) on the left side of the pipe operator and then the select() function on the right. The first argument is assumed to be the data, so the remaining argument inside the function, according to the documentation, should be one or more unquoted variable names or numbers representing the position of columns. If we put this together, the command looks like this: library(tidyverse) as_tibble(iris) %&gt;% select(Petal.Length, Sepal.Length, Species) #&gt; # A tibble: 150 x 3 #&gt; Petal.Length Sepal.Length Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 1.4 5.1 setosa #&gt; 2 1.4 4.9 setosa #&gt; 3 1.3 4.7 setosa #&gt; 4 1.5 4.6 setosa #&gt; 5 1.4 5 setosa #&gt; 6 1.7 5.4 setosa #&gt; # ... with 144 more rows The pipe is the glue for putting together multiple data transformations into a sequence. The benefits to using the pipe are that it reduces the amount of code needed to perform a sequence of operations. "],
["selecting.html", "4.2 Selecting", " 4.2 Selecting It is common to have a dataset with more columns than you need or have helper columns that you’ve created but no longer need. In the section above we demonstrated how to use the select() function with the pipe operator to pull out only the columns you want to work with. The select function has the nice feature of also being able to rename columns as you select them. For example, as_tibble(iris) %&gt;% select(petal_length=Petal.Length, sepal_lenth=Sepal.Length, Species) #&gt; # A tibble: 150 x 3 #&gt; petal_length sepal_lenth Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 1.4 5.1 setosa #&gt; 2 1.4 4.9 setosa #&gt; 3 1.3 4.7 setosa #&gt; 4 1.5 4.6 setosa #&gt; 5 1.4 5 setosa #&gt; 6 1.7 5.4 setosa #&gt; # ... with 144 more rows There is another function in dplyr called rename(), however, this can conflict with a function in the plyr package also named rename() if you have both packages loaded at the same time. To avoid confusion about which function to use you can reference the function you want using the package name and a double colon before the function like so, dplyr::rename(). This notation specifies the function and where to retrieve its definition, in this case, the dplyr package. If you would like to avoid this package function conflict, using just the select() function will help you accomplish everything and a little bit more than what the rename() function can do. The rename() function keeps all variables if you specify just one, then it will rename that one variable. By default, the select() function drops all other variables not specified, but you can work around this by supplying the everything() function into select(). The everything() select helper is not the only one. There are helper functions that match columns by name, numerical range, prefix, suffix and more. In the example below we are renaming the ID column, then pulling any of columns with “var” in the name, then everything else. as_tibble(iris) %&gt;% select(flower=Species, contains(&quot;Petal&quot;), everything()) #&gt; # A tibble: 150 x 5 #&gt; flower Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 setosa 1.4 0.2 5.1 3.5 #&gt; 2 setosa 1.4 0.2 4.9 3 #&gt; 3 setosa 1.3 0.2 4.7 3.2 #&gt; 4 setosa 1.5 0.2 4.6 3.1 #&gt; 5 setosa 1.4 0.2 5 3.6 #&gt; 6 setosa 1.7 0.4 5.4 3.9 #&gt; # ... with 144 more rows FIND EXAMPLE THAT MATCHES TEXT ABOVE IT SHOULD WE DISCUSS THE VARIANTS OF SELECT? The select_helpers make it convenient to specify many variables at once without having to type them all out individually. "],
["mutating.html", "4.3 Mutating", " 4.3 Mutating Mutating a variable means changing a variable in place or creating a new variable. You can think of this as modifying the dataset you provide to the function and the variables of the dataset are the part that is changing or “mutating”. In the following example we’ll show in one step how to change an existing variable and create a new one. as_tibble(iris) %&gt;% mutate(Species = paste(Species, &quot;flower&quot;), above_avg_sep_len = (Sepal.Length &gt; mean(iris$Sepal.Length))) #&gt; # A tibble: 150 x 6 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa… #&gt; 2 4.9 3 1.4 0.2 setosa… #&gt; 3 4.7 3.2 1.3 0.2 setosa… #&gt; 4 4.6 3.1 1.5 0.2 setosa… #&gt; 5 5 3.6 1.4 0.2 setosa… #&gt; 6 5.4 3.9 1.7 0.4 setosa… #&gt; # ... with 144 more rows, and 1 more variable: above_avg_sep_len &lt;lgl&gt; If you are a frequent user of Excel, the mutate() function is how you can implement the logic of an IF statement. In the example below we create a 0/1 indicator variable based on another column. as_tibble(iris) %&gt;% mutate(Species = paste(Species, &quot;flower&quot;), above_five_sep_len = ifelse(Sepal.Length &gt; 5, 1, 0)) #&gt; # A tibble: 150 x 6 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa… #&gt; 2 4.9 3 1.4 0.2 setosa… #&gt; 3 4.7 3.2 1.3 0.2 setosa… #&gt; 4 4.6 3.1 1.5 0.2 setosa… #&gt; 5 5 3.6 1.4 0.2 setosa… #&gt; 6 5.4 3.9 1.7 0.4 setosa… #&gt; # ... with 144 more rows, and 1 more variable: above_five_sep_len &lt;dbl&gt; In Excel you would implement this as a formula IF(A2 &gt; 5, 1, 0) in Row 2 of a new blank column and drag the formula down. Another common data transformation activity is recoding data. Instead of creating a series of nested IF statements in Excel or in R, you can leverage the recode() function inside of mutate(). In this example we are changing the survey response choice labels to numbers so that we can calculate summary statistics, like the mean and standard deviation of responses. # mutate(response = recode(response, # `Strongly Agree` = 6, # `Agree` = 5, # `Slightly Agree` = 4, # `Slightly Disagree` = 3, # `Disagree` = 2, # `Strongly Disagree` = 1)) If you have many survey questions on this scale and would like to them all in one pass, then you can implement with the mutate_at() function. It is an extension of the mutate() function that can apply your logic “at” multiple columns. # mutate_at(c(), funs(recode(., # `Strongly Agree` = 6, # `Agree` = 5, # `Slightly Agree` = 4, # `Slightly Disagree` = 3, # `Disagree` = 2, # `Strongly Disagree` = 1)) DECIDE IF WE HAVE AN EXAMPLE AND WANT TO SHOW MUTATE_IF() Aside from the mutate_at() function there is a mutate_if() function that will apply your logic “if” a column meets a certain criteria that you specify. This is a very handy function if, for example, you want to convert multiple columns that are character strings into integers. #mutate_if(is.character, as.numeric) "],
["grouped-mutates.html", "4.4 Grouped Mutates", " 4.4 Grouped Mutates A particularly challenging type of transformation in Excel is creating variables based on a group of records and applying it across all rows for the group. For example, calculating the difference between an individual’s maximum value compared to all other values. This requires finding the maximum for the group, then comparing it with each observation for the group. Typically, you might accomplish this through the creation of a Pivot Table and then using VLOOKUP to reference the maximum per individual. This gets unwieldly (sp) when then data changes. In R you can take the observations in a dataset and put them into groups based on a variable. In the example below, we have simply taken the dataset and modified it to remember that operations should be done in their respective group. Nothing has actually been changed on the data yet, but it is ready for an operation that acts upon the grouped data. The next step is to create that variable we mentioned above that represents the difference between the maximum value for the individual and each of their corresponding observations. In this example, we’ll also create another variable indicating TRUE/FALSE whether the row is the maximum to provide a visual cue and filtering mechanism to throw out those specific observations if desired. # dat %&gt;% # group_by() "],
["summarizing.html", "4.5 Summarizing", " 4.5 Summarizing In the spirit of transforming data we will talk about summarizing it. A summary is just a transformation of underlying source data to a new form. Before summarizing data you must first have an idea about how to summarize it. Ask yourself: Do I want to count rows? Unique instances? Do I want to calculate the average? Standard deviation? Excel users often summarize using a Pivot Table which has the summarizing functions COUNT, COUNTA, AVERAGE, STDEV, MAX, and MIN to name a few. These are very similar to what exists in R and below is a table that translates each of these summary methods to their equivalent counterpart in R. [INSERT TABLE HERE] Below is an example in R of how to count the number of observations and compute the average. as_tibble(iris) %&gt;% summarise(n = n(), mean=mean(Petal.Length)) #&gt; # A tibble: 1 x 2 #&gt; n mean #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 150 3.76 If you would like to compute this same summary for different groups of the data then you can just add the group_by() function. As mentioned above, the group_by() function simply prepares the dataset so that it is segmented. Adding the summarize() function after the group_by means that the operations will be run separately across each of the data segments to compute one summary for each. as_tibble(iris) %&gt;% group_by(Species) %&gt;% summarise(n = n(), mean=mean(Petal.Length)) #&gt; # A tibble: 3 x 3 #&gt; Species n mean #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 setosa 50 1.46 #&gt; 2 versicolor 50 4.26 #&gt; 3 virginica 50 5.55 In Excel, the alternative to using a Pivot Table to summarize data for a group might be to use the COUNTIF, SUMIF, AVERAGEIF functions. These functions perform the same summary methods as mentioned above, but only on rows that meet some criteria. This is better represented in an R pipeline by using the filter() function to only select the observations that meet your criteria and then summarizing. In the example below we remove X observations and then calculate Y. as_tibble(iris) %&gt;% filter(Species == &#39;setosa&#39;) %&gt;% summarise(n = n(), mean=mean(Petal.Length)) #&gt; # A tibble: 1 x 2 #&gt; n mean #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 50 1.46 In Excel you would implement this as a formula AVERAGEIF(E2:E150, &quot;setosa&quot;, A2:A150). To reiterate, in R filtering just means that you remove any rows from a dataset that meet a certain criteria. This is helpful in a pipeline because it will remove the observations while performing that pipeline calculation without removing them from the original dataset you placed at the top of the pipeline. The symbols for doing comparisons, also called “logical operators”, are pretty much the same in Excel as they are in R. One key difference is that if you are checking whether two things are equal to each other in R, then you will need to use double equal signs (A2==B2) rather than the single equals sign that Excel uses A2=B2. Below is a table that shows how these operators are different in R compared to Excel [INSERT TABLE HERE] In the example below we show multiple of these operators in action to show how they can be used together to select a very small subset of observations from a data set. as_tibble(iris) %&gt;% filter(Species == &#39;setosa&#39;, !(Petal.Length == 1.4 | Petal.Length == 1.5), Petal.Width &lt; 0.2) %&gt;% summarise(n = n()) #&gt; # A tibble: 1 x 1 #&gt; n #&gt; &lt;int&gt; #&gt; 1 1 "],
["spread-and-gather.html", "4.6 Spread and Gather", " 4.6 Spread and Gather One particular type of transformation is moving the data between a “long” format and a “wide” format. In the wide format each metric might be in its own column. If you have 20 metrics then the dataset will have at least 20 columns and it starts to become wider than it is long. FIGURE OUT AN EXAMPLE THAT IS REALLY REPRESENTATIVE OF THIS [INCLUDE LONG EXAMPLE] *** The long format is usually more preferable because functions like filter(), group_by(), and summarize() can operate quickly across rows rather than columns. In addition, the long format is more compatible with the ggplot() plotting function to specify multiple series, colors, and facets. SHOULD WE SHOW AN EXAMPLE OF GGPLOT HERE? OTHER IDEAS/REASONS *** "],
["functions.html", "4.7 Functions", " 4.7 Functions In some cases "],
["putting-it-all-together.html", "4.8 Putting it all together", " 4.8 Putting it all together In this section we will show an example that utilizes many of the different methods described in this chapter so that readers can have a complete example "],
["relational.html", "5 Relational Data ", " 5 Relational Data "],
["tidy-principles.html", "5.1 Tidy Principles", " 5.1 Tidy Principles [INSERT QUIP ABOUT TIDY DATA OR QUOTE] In Chapter 2 when we introduced the R programming language as our preferred tool for marketing analytics we discussed the concept of “tidy” data. More in depth about tidy principles DESCRIBE WHAT IS A TIBBLE VS DATAFRAME? How to store the data (what is a row and what is a column) "],
["relational-structures.html", "5.2 Relational Structures", " 5.2 Relational Structures data integrity, the role of keys, ER diagram revisit spread, gather conversation with normalization backdrop "],
["joins.html", "5.3 Joins", " 5.3 Joins If database architects “normalize” a database by creating multiple tables that reference each other, then we need a way to bring the data back together from each of these sources. The process of combining columns from two or more tables is called a “join”. “Joining” tables may also be described as “merging”. These phrases usually mean the same thing. The tricky part about joining datasets is understanding how observations in each set should be matched and how to handle instances where the data is missing from one of the tables. One of the most effective ways to learn the behavior of different joins is to use venn diagrams. Below is a comprehensive outline of join operations. You will notice that depending on the type of join certain records will be retained. For example, the inner_join() function will only retain records if they both exist in the dataset. [INSERT IMAGE HERE] You will also notice in the join reference above how the observations are matched. Typically, you will choose one column and whereever there is a match between the datasets, then the rows will be combined having now columns from both datasets together for the same observation. The one or more columns used to match records is typically called the join “key”. Consider a simple example where we have one dataset with three columns, the first is a record ID column and a second dataset with 2 columns, its first column is also a record ID column. We will assume the majority of records should have matching IDs between the two datasets. If we would like to “join” the second dataset into the first, then we should use the left_join() function. This function takes the first dataset provided, keeping all of its rows regardless of whether a matching record is found in the second dataset. All of the columns in the second dataset are now added to the first dataset. In this example, that means only one additional column. The ID column is not duplicated after the merge because that information is reflected in the ID column from the first dataset. For readers with knowledge of SQL programming, the join functions in R perform exactly as their counterparts in SQL. For readers with knowledge of Excel this is akin to using the VLOOKUP function or a combination of INDEX-MATCH to pull records from another dataset. SHOULD WE EXPLICITLY SHOW THE TRANSLATION FROM VLOOKUP TO LEFT_JOIN()? OR INDEX-MATCH? *** As depicted in the joins reference diagram there are different join types for returning records from both datasets, all from one dataset, or only the overalapping records. For example, the outer_join() function combines the rows that match on the join key(s) and retains all of the unmatched rows from each dataset. This is also sometimes referred to as a “full join” since it retains the full set of records from both datasets. One join function that is unique is the anti_join() function. This join returns only the rows that are not matched in the second dataset. This is particularly useful for diagnosing what is not matching. If you think about its behavior, it should, together with the inner_join() function span all the records in the first dataset you provide. This is because either the records are matched via inner_join() or they are not matched and identifed via anti_join(). Below is an example which highlights that 95% of records are matched between the two datasets. The remaining 5% are returned via the anti_join() function for us to further inspect. "],
["measures.html", "6 Measures in Marketing", " 6 Measures in Marketing The major topic areas are provided below. "],
["net-promoter-score.html", "6.1 Net Promoter Score", " 6.1 Net Promoter Score Other measure types Likert Scale, Distributionally, Mean, Top 2 Box "],
["customer-lifetime-value.html", "6.2 Customer Lifetime Value", " 6.2 Customer Lifetime Value "],
["customer-satisfactioncustomer-delight.html", "6.3 Customer Satisfaction/Customer Delight", " 6.3 Customer Satisfaction/Customer Delight "],
["others-consult-ama-dictionary-for-more.html", "6.4 Others? Consult AMA dictionary for more?", " 6.4 Others? Consult AMA dictionary for more? "],
["fieldexp.html", "7 Conducting Marketing Field Experiment", " 7 Conducting Marketing Field Experiment THINGS PROVIDED BELOW THIS LINE ARE FOR EXAMPLE ONLY. NOT REAL CONTENT We have finished a nice book. "],
["mix.html", "8 Understanding Marketing Mix", " 8 Understanding Marketing Mix THINGS PROVIDED BELOW THIS LINE ARE FOR EXAMPLE ONLY. NOT REAL CONTENT We have finished a nice book. "],
["trade.html", "9 Pricing and Merchandising", " 9 Pricing and Merchandising THINGS PROVIDED BELOW THIS LINE ARE FOR EXAMPLE ONLY. NOT REAL CONTENT We have finished a nice book. "],
["machinelearning.html", "10 Machine Learning in Marketing", " 10 Machine Learning in Marketing THINGS PROVIDED BELOW THIS LINE ARE FOR EXAMPLE ONLY. NOT REAL CONTENT We have finished a nice book. "],
["action.html", "11 Moving from Data to Action", " 11 Moving from Data to Action THINGS PROVIDED BELOW THIS LINE ARE FOR EXAMPLE ONLY. NOT REAL CONTENT We have finished a nice book. "]
]
